{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb65ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6f7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a Spark session with AWS Support.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4b8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark):\n",
    "    \"\"\"\n",
    "    Load the data from song-data.zip to create the songs and artists tables\n",
    "    to the star schema. Also, We used the spark functions to obtain the columns\n",
    "    required. This data will be write and load in a S3 Bucket in parquet format.\n",
    "    \n",
    "    Args:\n",
    "        spark: Instantiation of spark session.\n",
    "        input_data = Path to the song-data s3 bucket.\n",
    "        output_data = Path to store the parquet files.\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join('song_data/*/*/*/*.json')\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(['song_id', 'title', 'artist_id', 'year', 'duration'])\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet('song.parquet', partitionBy = ['year', 'artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location',\n",
    "                              'artist_latitude', 'artist_longitude') \\\n",
    "                                  .withColumnRenamed('artist_name', 'name') \\\n",
    "                                  .withColumnRenamed('artist_location', 'location') \\\n",
    "                                  .withColumnRenamed('artist_latitude', 'latitude') \\\n",
    "                                  .withColumnRenamed('artist_longitude', 'longitude') \\\n",
    "                                 .dropDuplicates()\n",
    "    artists_table.createOrReplaceTempView('artists')\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet('artists.parquet', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3907c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelandr/Desktop/data-lake-project-resources'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f12f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark):\n",
    "    \"\"\"\n",
    "    Load the data from log-data.zip to create the users,time and songplays tables\n",
    "    to the star schema. Also, We used the spark functions to obtain the columns\n",
    "    required. This data will be write and load in a S3 Bucket in parquet format.    \n",
    "    \n",
    "    Args:\n",
    "        spark: Instantiation of spark session.\n",
    "        input_data = Path to the song-data s3 bucket.\n",
    "        output_data = Path to store the parquet files.    \n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = 'log-data/*.json'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    actions_df = df.where(df.page == 'NextSong').select('ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').dropDuplicates()\n",
    "    users_table.createOrReplaceTempView('users')\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet('users/users.parquet', 'overwrite')\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: str(int(int(x)/1000)))\n",
    "    actions_df = actions_df.withColumn('timestamp', get_timestamp(actions_df.ts))       \n",
    "   \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000)))\n",
    "    actions_df = actions_df.withColumn('datetime', get_datetime(actions_df.ts))\n",
    "       \n",
    "    # extract columns to create time table\n",
    "    time_table = actions_df.select('datetime') \\\n",
    "                           .withColumn('start_time', actions_df.datetime) \\\n",
    "                           .withColumn('hour', hour('datetime')) \\\n",
    "                           .withColumn('day', dayofmonth('datetime')) \\\n",
    "                           .withColumn('week', weekofyear('datetime')) \\\n",
    "                           .withColumn('month', month('datetime')) \\\n",
    "                           .withColumn('year', year('datetime')) \\\n",
    "                           .withColumn('weekday', dayofweek('datetime')) \\\n",
    "                           .dropDuplicates()\n",
    "\n",
    "        # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month') \\\n",
    "                    .parquet('time/time.parquet', 'overwrite')\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json('song_data/*/*/*/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    actions_df = actions_df.alias('log_df')\n",
    "    song_df = song_df.alias('song_df')\n",
    "    joined_df = actions_df.join(song_df, col('log_df.artist') == col('song_df.artist_name'), 'inner')\n",
    "    songplays_table = joined_df.select(\n",
    "        col('log_df.datetime').alias('start_time'),\n",
    "        col('log_df.userId').alias('user_id'),\n",
    "        col('log_df.level').alias('level'),\n",
    "        col('song_df.song_id').alias('song_id'),\n",
    "        col('song_df.artist_id').alias('artist_id'),\n",
    "        col('log_df.sessionId').alias('session_id'),\n",
    "        col('log_df.location').alias('location'), \n",
    "        col('log_df.userAgent').alias('user_agent'),\n",
    "        year('log_df.datetime').alias('year'),\n",
    "        month('log_df.datetime').alias('month')) \\\n",
    "        .withColumn('songplay_id', monotonically_increasing_id())\n",
    "\n",
    "    songplays_table.createOrReplaceTempView('songplays')\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    time_table = time_table.alias('timetable')\n",
    "\n",
    "    songplays_table.write.partitionBy('year', 'month').parquet( 'songplays/songplays.parquet', 'overwrite')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7b0b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/michaelandr/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/michaelandr/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/michaelandr/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-67c03752-5d3f-479b-9712-f764ebed3778;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.0 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.11.1 in central\n",
      "\t[2.11.1] joda-time#joda-time;[2.2,)\n",
      ":: resolution report :: resolve 5756ms :: artifacts dl 55ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.1 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.11.1 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   1   |   0   |   0   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: retrieving :: org.apache.spark#spark-submit-parent-67c03752-5d3f-479b-9712-f764ebed3778\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/33ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 14:55:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    process_song_data(spark)#, input_data, output_data)    \n",
    "    process_log_data(spark)#, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bb58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = create_spark_session()\n",
    "# data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "#               (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "#               (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "#               (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "#               (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "# columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "#df = spark.read.json('log-data/*.json')\n",
    "\n",
    "# df=spark.createDataFrame(data,columns)\n",
    "# df.write.parquet(\"song.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aba6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n",
    "#df = df.where(df.page == 'NextSong').select('ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')\n",
    "#'ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')\n",
    "#users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').dropDuplicates()\n",
    "#users_table.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104ea275",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xh/l9ggclrd1fx2brvxlnc_9hv40000gn/T/ipykernel_998/3726558592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
