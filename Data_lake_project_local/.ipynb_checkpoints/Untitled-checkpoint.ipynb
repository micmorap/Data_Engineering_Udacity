{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb65ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bf7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "\n",
    "#os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6f7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a Spark session with AWS Support.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4b8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark):\n",
    "    \"\"\"\n",
    "    Load the data from song-data.zip to create the songs and artists tables\n",
    "    to the star schema. Also, We used the spark functions to obtain the columns\n",
    "    required. This data will be write and load in a S3 Bucket in parquet format.\n",
    "    \n",
    "    Args:\n",
    "        spark: Instantiation of spark session.\n",
    "        input_data = Path to the song-data s3 bucket.\n",
    "        output_data = Path to store the parquet files.\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join('song_data/*/*/*/*.json')\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(['song_id', 'title', 'artist_id', 'year', 'duration'])\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.parquet('song.parquet', partitionBy = ['year', 'artist_id'])\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location',\n",
    "                              'artist_latitude', 'artist_longitude') \\\n",
    "                                  .withColumnRenamed('artist_name', 'name') \\\n",
    "                                  .withColumnRenamed('artist_location', 'location') \\\n",
    "                                  .withColumnRenamed('artist_latitude', 'latitude') \\\n",
    "                                  .withColumnRenamed('artist_longitude', 'longitude') \\\n",
    "                                 .dropDuplicates()\n",
    "    artists_table.createOrReplaceTempView('artists')\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet('artists.parquet', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3907c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelandr/Desktop/data-lake-project-resources'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f12f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark):\n",
    "    \"\"\"\n",
    "    Load the data from log-data.zip to create the users,time and songplays tables\n",
    "    to the star schema. Also, We used the spark functions to obtain the columns\n",
    "    required. This data will be write and load in a S3 Bucket in parquet format.    \n",
    "    \n",
    "    Args:\n",
    "        spark: Instantiation of spark session.\n",
    "        input_data = Path to the song-data s3 bucket.\n",
    "        output_data = Path to store the parquet files.    \n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = 'log-data/*.json'\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    actions_df = df.where(df.page == 'NextSong').select('ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').dropDuplicates()\n",
    "    users_table.createOrReplaceTempView('users')\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.parquet('users/users.parquet', 'overwrite')\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: str(int(int(x)/1000)))\n",
    "    actions_df = actions_df.withColumn('timestamp', get_timestamp(actions_df.ts))       \n",
    "   \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000)))\n",
    "    actions_df = actions_df.withColumn('datetime', get_datetime(actions_df.ts))\n",
    "       \n",
    "    # extract columns to create time table\n",
    "    time_table = actions_df.select('datetime') \\\n",
    "                           .withColumn('start_time', actions_df.datetime) \\\n",
    "                           .withColumn('hour', hour('datetime')) \\\n",
    "                           .withColumn('day', dayofmonth('datetime')) \\\n",
    "                           .withColumn('week', weekofyear('datetime')) \\\n",
    "                           .withColumn('month', month('datetime')) \\\n",
    "                           .withColumn('year', year('datetime')) \\\n",
    "                           .withColumn('weekday', dayofweek('datetime')) \\\n",
    "                           .dropDuplicates()\n",
    "\n",
    "        # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month') \\\n",
    "                    .parquet('time/time.parquet', 'overwrite')\n",
    "    \n",
    "    # read in song data to use for songplays table\n",
    "    song_df = spark.read.json('song_data/*/*/*/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    actions_df = actions_df.alias('log_df')\n",
    "    song_df = song_df.alias('song_df')\n",
    "    joined_df = actions_df.join(song_df, col('log_df.artist') == col('song_df.artist_name'), 'inner')\n",
    "    songplays_table = joined_df.select(\n",
    "        col('log_df.datetime').alias('start_time'),\n",
    "        col('log_df.userId').alias('user_id'),\n",
    "        col('log_df.level').alias('level'),\n",
    "        col('song_df.song_id').alias('song_id'),\n",
    "        col('song_df.artist_id').alias('artist_id'),\n",
    "        col('log_df.sessionId').alias('session_id'),\n",
    "        col('log_df.location').alias('location'), \n",
    "        col('log_df.userAgent').alias('user_agent'),\n",
    "        year('log_df.datetime').alias('year'),\n",
    "        month('log_df.datetime').alias('month')) \\\n",
    "        .withColumn('songplay_id', monotonically_increasing_id())\n",
    "\n",
    "    songplays_table.createOrReplaceTempView('songplays')\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    time_table = time_table.alias('timetable')\n",
    "\n",
    "    songplays_table.write.partitionBy('year', 'month').parquet( 'songplays/songplays.parquet', 'overwrite')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7b0b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/michaelandr/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/michaelandr/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/michaelandr/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-458e4934-814a-4e9f-9266-3e557a13f22d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;2.7.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.7.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound xmlenc#xmlenc;0.52 in central\n",
      "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.4 in central\n",
      "\tfound commons-io#commons-io;2.4 in central\n",
      "\tfound commons-net#commons-net;3.1 in central\n",
      "\tfound commons-collections#commons-collections;3.2.1 in central\n",
      "\tfound javax.servlet#servlet-api;2.5 in central\n",
      "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
      "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.9 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.9 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
      "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
      "\tfound javax.activation#activation;1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.9 in central\n",
      "\tfound asm#asm;3.2 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.2.5 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.2.5 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound commons-configuration#commons-configuration;1.6 in central\n",
      "\tfound commons-digester#commons-digester;1.8 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.7.0 in central\n",
      "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.10 in central\n",
      "\tfound org.apache.avro#avro;1.7.4 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.0.4.1 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;2.7.0 in central\n",
      "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
      "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
      "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.6 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.10 in central\n",
      "\tfound io.netty#netty;3.6.2.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.7.1 in central\n",
      "\tfound org.apache.curator#curator-client;2.7.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.42 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.7.1 in central\n",
      "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound jline#jline;0.9.94 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.2.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.2.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk;1.7.4 in central\n",
      "\tfound joda-time#joda-time;2.11.1 in central\n",
      "\t[2.11.1] joda-time#joda-time;[2.2,)\n",
      ":: resolution report :: resolve 6245ms :: artifacts dl 42ms\n",
      "\t:: modules in use:\n",
      "\tasm#asm;3.2 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk;1.7.4 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.2.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.2.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.9 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.9 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.7.0 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.4 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.1 from central in [default]\n",
      "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
      "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
      "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
      "\tcommons-io#commons-io;2.4 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.1 from central in [default]\n",
      "\tio.netty#netty;3.6.2.Final from central in [default]\n",
      "\tjavax.activation#activation;1.1 from central in [default]\n",
      "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
      "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
      "\tjline#jline;0.9.94 from central in [default]\n",
      "\tjoda-time#joda-time;2.11.1 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.4 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.7.1 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.7.1 from central in [default]\n",
      "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
      "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;2.7.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;2.7.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.2.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.2.5 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.6 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
      "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.10 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.10 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.4.1 from central in [default]\n",
      "\txmlenc#xmlenc;0.52 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   1   |   0   |   0   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-458e4934-814a-4e9f-9266-3e557a13f22d\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0 artifacts copied, 68 already retrieved (0kB/34ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 10:28:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    #input_data = \"s3a://udacity-dend/\"\n",
    "    #output_data = \"s3a://mm-dl-loaded-data/\"\n",
    "    \n",
    "    #process_song_data(spark)#, input_data, output_data)    \n",
    "    process_log_data(spark)#, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bb58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = create_spark_session()\n",
    "# data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "#               (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "#               (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "#               (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "#               (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "# columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "#df = spark.read.json('log-data/*.json')\n",
    "\n",
    "# df=spark.createDataFrame(data,columns)\n",
    "# df.write.parquet(\"song.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aba6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n",
    "#df = df.where(df.page == 'NextSong').select('ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')\n",
    "#'ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')\n",
    "#users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').dropDuplicates()\n",
    "#users_table.createOrReplaceTempView('users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104ea275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|              artist|      auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+--------------------+----------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|            Harmonia| Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|\n",
      "|         The Prodigy| Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|     The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|\n",
      "|               Train| Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|            Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|\n",
      "|                null| Logged In|    Wyatt|     M|            0|   Scott|     null| free|Eureka-Arcata-For...|   GET|    Home|1.540872073796E12|      563|                null|   200|1542247071796|Mozilla/5.0 (Wind...|     9|\n",
      "|                null| Logged In|   Austin|     M|            0| Rosales|     null| free|New York-Newark-J...|   GET|    Home|1.541059521796E12|      521|                null|   200|1542252577796|Mozilla/5.0 (Wind...|    12|\n",
      "|         Sony Wonder| Logged In|   Samuel|     M|            0|Gonzalez|218.06975| free|Houston-The Woodl...|   PUT|NextSong|1.540492941796E12|      597|           Blackbird|   200|1542253449796|\"Mozilla/5.0 (Mac...|    61|\n",
      "|                null| Logged In|   Samuel|     M|            1|Gonzalez|     null| free|Houston-The Woodl...|   GET|   About|1.540492941796E12|      597|                null|   200|1542253460796|\"Mozilla/5.0 (Mac...|    61|\n",
      "|                null|Logged Out|     null|  null|            0|    null|     null| paid|                null|   PUT|   Login|             null|      602|                null|   307|1542260074796|                null|      |\n",
      "|                null| Logged In|    Tegan|     F|            1|  Levine|     null| paid|Portland-South Po...|   GET|    Home|1.540794356796E12|      602|                null|   200|1542260277796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|           Van Halen| Logged In|    Tegan|     F|            2|  Levine|289.38404| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|Best Of Both Worl...|   200|1542260935796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|           Magic Sam| Logged In|    Tegan|     F|            3|  Levine|132.04853| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|Call Me If You Ne...|   200|1542261224796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|Edward Sharpe & T...| Logged In|    Tegan|     F|            4|  Levine|306.31138| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|                Home|   200|1542261356796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|Usher featuring w...| Logged In|    Tegan|     F|            5|  Levine|395.72853| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|                 OMG|   200|1542261662796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|                null| Logged In|    Tegan|     F|            6|  Levine|     null| paid|Portland-South Po...|   GET|    Home|1.540794356796E12|      602|                null|   200|1542261713796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|         Helen Reddy| Logged In|    Tegan|     F|            7|  Levine|176.50893| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602| Candle On The Water|   200|1542262057796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|        Taylor Swift| Logged In|    Tegan|     F|            8|  Levine|201.06404| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|            Our Song|   200|1542262233796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|           Sean Paul| Logged In|    Tegan|     F|            9|  Levine|245.34159| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|Baby Boy [feat. B...|   200|1542262434796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|         Soundgarden| Logged In|     Lily|     F|            0|    Koch|272.19546| paid|Chicago-Napervill...|   PUT|NextSong|1.541048010796E12|      582|      Black Hole Sun|   200|1542262456796|\"Mozilla/5.0 (X11...|    15|\n",
      "|         The Killers| Logged In|    Tegan|     F|           10|  Levine|360.75057| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|               Human|   200|1542262679796|\"Mozilla/5.0 (Mac...|    80|\n",
      "|       Amy Winehouse| Logged In|     Lily|     F|            1|    Koch|165.11955| paid|Chicago-Napervill...|   PUT|NextSong|1.541048010796E12|      582|            Addicted|   200|1542262728796|\"Mozilla/5.0 (X11...|    15|\n",
      "+--------------------+----------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
