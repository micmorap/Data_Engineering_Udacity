{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages required:\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Introduction, datasets and scope of the project:\n",
    "\n",
    "The main objective from this first project is apply all the concepts learned along the Data Engineering Nanodegree. I work\n",
    "with four datasets provided by Udacity. Below, you can see a brief description over each one of datasets:\n",
    "\n",
    "1. __I94 Immigration Data:__ This dataset was provided by US National Tourism and Trade Office. The aim of this information is create tools to positive growth in travel and tourism along the country and its economy and employment. Moreover, this report provides statistics and insights on KPIs about tourists and non-residents visitants to the US. In the next links you can find general information about this public entity and a dashboard made on Power BI with multiple statistics, indicators per visa types, port of entry, age groups, among others.\n",
    "\n",
    "\n",
    "2. __World Temperature Data:__ Kaggle is the source of this dataset. The authors of the data (from Berkeley Earth Laboratory), combined 1.6 billion reports around the world. A huge amount of this information was collected by special technicians using mercury and electronic thermometers. The result was global, land and ocean temperaturesand even per countries and states. To a deep explanation about the structure of the data, you can visit the next URL:\n",
    "\n",
    "\n",
    "3. __US City Demographic Data__ Opendatasoft built this information to all cities in United States with a population greater or equal to 65.000 people. You can find the info in csv, json and excel format and even an API to requests information. You can find in the next URL, a description, visualizations and descriptions about this dataset.\n",
    "\n",
    "\n",
    "4. __Airport Code Table:__ This data was provided by datahub.io and get the information about the airport code with the international standards (IATA Code, that's mean a three-letter code with their corresponding geographical data). You can find the basic data modeling of this info and the download option (either json or csv extension through multiple programming languages as Python or R)in the next link:\n",
    "\n",
    "\n",
    "The scope of this project is apply all the knowledges learned along the nanodegree. The main purpose will be build a data warehouse with a fact and dimension tables according to the data explained above to help to analytics teams and making decision as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "We're going to review the data quality in each one the datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we can read, explore and clean the corresponding datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 I94 Inmigration Data: Exploratory data analysis and cleaning:\n",
    "\n",
    "To understand the structure and the definitions of the columns you can open the ___I94_SAS_Labels_Descriptions.SAS___ contained in this github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the spark session: \n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the folder path from parquet files:\n",
    "parquet_path = os.path.join(os.getcwd() + '/sas_data/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the parquet files in the path indicated previously:\n",
    "df_spark = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we can look one sample of the data:\n",
    "df_spark.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[cicid: double, i94yr: double, i94mon: double, i94cit: double, i94res: double, i94port: string, arrdate: double, i94mode: double, i94addr: string, depdate: double, i94bir: double, i94visa: double, count: double, dtadfile: string, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, biryear: double, dtaddto: string, gender: string, insnum: string, airline: string, admnum: double, fltno: string, visatype: string]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the schema to understand the data modeling and how was defined each column per data type:\n",
    "df_spark.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According the ___I94_SAS_Labels_Descriptions.SAS___ file, we'll apply the next transformations:\n",
    "\n",
    "* Format the date to ARRDATE (arrival date in USA), DEPDATE (Departure date from USA), DTADDTO (Date to which admitted to US) columns\n",
    "* Drop the next columns:\n",
    "    * DTADFILE (unknown description)\n",
    "    * VISAPOST (Departmanet of State where Visa was issued)\n",
    "    * OCCUP (Occupation)\n",
    "    * ENTDEPA (admitted or paroled into the U.S.)\n",
    "    * ENTDEPD (Departed, lost I-94 or is deceased)\n",
    "    * ENTDEPU (Update Flag - Either apprehended, overstayed, adjusted to perm residence)\n",
    "    \n",
    "    Every column doesn't use inside the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 World Temperature Data:\n",
    "\n",
    "To understand the structure and the definitions of the columns you can open the next link \n",
    "[climate-change-earth-surface-temperature-data](www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data) contained in Kaggle webpage. To this project, we take the GlobalLandTemperaturesByState in csv format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelandr/Desktop/Data_Engineering_Udacity/Capstone_Project/global_temperatures/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the current path to global temperatures per state: \n",
    "data_temperatures_path = os.path.join(os.getcwd() + \"/global_temperatures/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv dataset\n",
    "data_temperatures = pd.read_csv(data_temperatures_path + 'GlobalLandTemperaturesByState.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1855-05-01</td>\n",
       "      <td>25.544</td>\n",
       "      <td>1.171</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1855-06-01</td>\n",
       "      <td>24.228</td>\n",
       "      <td>1.103</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1855-07-01</td>\n",
       "      <td>24.371</td>\n",
       "      <td>1.044</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1855-08-01</td>\n",
       "      <td>25.427</td>\n",
       "      <td>1.073</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1855-09-01</td>\n",
       "      <td>25.675</td>\n",
       "      <td>1.014</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty State Country\n",
       "0  1855-05-01              25.544                          1.171  Acre  Brazil\n",
       "1  1855-06-01              24.228                          1.103  Acre  Brazil\n",
       "2  1855-07-01              24.371                          1.044  Acre  Brazil\n",
       "3  1855-08-01              25.427                          1.073  Acre  Brazil\n",
       "4  1855-09-01              25.675                          1.014  Acre  Brazil"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We look a couple of records:\n",
    "data_temperatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>620027.000000</td>\n",
       "      <td>620027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.993111</td>\n",
       "      <td>1.287647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.772150</td>\n",
       "      <td>1.360392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-45.389000</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.693000</td>\n",
       "      <td>0.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.199000</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.899000</td>\n",
       "      <td>1.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36.339000</td>\n",
       "      <td>12.646000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AverageTemperature  AverageTemperatureUncertainty\n",
       "count       620027.000000                  620027.000000\n",
       "mean             8.993111                       1.287647\n",
       "std             13.772150                       1.360392\n",
       "min            -45.389000                       0.036000\n",
       "25%             -0.693000                       0.316000\n",
       "50%             11.199000                       0.656000\n",
       "75%             19.899000                       1.850000\n",
       "max             36.339000                      12.646000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see any statistical results along the dataset\n",
    "data_temperatures.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 645675 entries, 0 to 645674\n",
      "Data columns (total 5 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   dt                             645675 non-null  object \n",
      " 1   AverageTemperature             620027 non-null  float64\n",
      " 2   AverageTemperatureUncertainty  620027 non-null  float64\n",
      " 3   State                          645675 non-null  object \n",
      " 4   Country                        645675 non-null  object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 24.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Info about the columns data types:\n",
    "data_temperatures.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll apply the next transformations to clean the dataset:\n",
    "\n",
    "* Convert in date format the dt column\n",
    "* Extract the year from dt column and create the respective column\n",
    "* Drop the AverageTemperatureUncertainty columns,because we can work AverageTemperature without problems.\n",
    "* Filter the dataframe only to United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 US City Demographic Data:\n",
    "\n",
    "This data comes from the US Census Bureau's 2015 American Community Survey detailed in the next below. To understand the structure and the definitions of the columns you can open the next link \n",
    "[US Cities: Demographics](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/?dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6InVzLWNpdGllcy1kZW1vZ3JhcGhpY3MiLCJvcHRpb25zIjp7fX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImNvbHVtbiIsImZ1bmMiOiJBVkciLCJ5QXhpcyI6Im1lZGlhbl9hZ2UiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjRkY1MTVBIn1dLCJ4QXhpcyI6ImNpdHkiLCJtYXhwb2ludHMiOjUwLCJzb3J0IjoiIn1dLCJ0aW1lc2NhbGUiOiIiLCJkaXNwbGF5TGVnZW5kIjp0cnVlLCJhbGlnbk1vbnRoIjp0cnVlfQ%3D%3D) contained in Opensoft webpage. To this project, we take the info in json format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the current path to US demographic info: \n",
    "demographic_path = os.path.join(os.getcwd() + \"/us_demographics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json dataset\n",
    "with open(demographic_path + 'us-cities-demographics.json', 'r') as f:\n",
    "    dataset_demographics = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, normalize the dataset:\n",
    "data_demographics = pd.json_normalize(dataset_demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasetid</th>\n",
       "      <th>recordid</th>\n",
       "      <th>fields.total_population</th>\n",
       "      <th>fields.female_population</th>\n",
       "      <th>fields.count</th>\n",
       "      <th>fields.foreign_born</th>\n",
       "      <th>fields.state_code</th>\n",
       "      <th>fields.average_household_size</th>\n",
       "      <th>fields.city</th>\n",
       "      <th>fields.race</th>\n",
       "      <th>fields.male_population</th>\n",
       "      <th>fields.median_age</th>\n",
       "      <th>fields.number_of_veterans</th>\n",
       "      <th>fields.state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>a53c89d37b41991249c2a00b25c61c4a3ba692d3</td>\n",
       "      <td>682545</td>\n",
       "      <td>341408.0</td>\n",
       "      <td>72288</td>\n",
       "      <td>113222.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>2.33</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>341137.0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>29363.0</td>\n",
       "      <td>Colorado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>ff1f3fda43e43418dd425119d10b14094ce56c9c</td>\n",
       "      <td>115258</td>\n",
       "      <td>59027.0</td>\n",
       "      <td>1916</td>\n",
       "      <td>10925.0</td>\n",
       "      <td>UT</td>\n",
       "      <td>3.28</td>\n",
       "      <td>Provo</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>56231.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>2177.0</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>d47679da2b529ad33f9991204c5857d0fc8e1bdb</td>\n",
       "      <td>136454</td>\n",
       "      <td>70240.0</td>\n",
       "      <td>7513</td>\n",
       "      <td>6204.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>2.48</td>\n",
       "      <td>Hampton</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>66214.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>19638.0</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>cc485012a82ee94e8f930e0876721b46cac8dadb</td>\n",
       "      <td>214911</td>\n",
       "      <td>112789.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>8258.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>2.21</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>Asian</td>\n",
       "      <td>102122.0</td>\n",
       "      <td>35.6</td>\n",
       "      <td>13212.0</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us-cities-demographics</td>\n",
       "      <td>b99c18f8d5b344bd820a2606e57149a6ab91af26</td>\n",
       "      <td>100883</td>\n",
       "      <td>50091.0</td>\n",
       "      <td>92874</td>\n",
       "      <td>11480.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Greeley</td>\n",
       "      <td>White</td>\n",
       "      <td>50792.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4294.0</td>\n",
       "      <td>Colorado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                datasetid                                  recordid  \\\n",
       "0  us-cities-demographics  a53c89d37b41991249c2a00b25c61c4a3ba692d3   \n",
       "1  us-cities-demographics  ff1f3fda43e43418dd425119d10b14094ce56c9c   \n",
       "2  us-cities-demographics  d47679da2b529ad33f9991204c5857d0fc8e1bdb   \n",
       "3  us-cities-demographics  cc485012a82ee94e8f930e0876721b46cac8dadb   \n",
       "4  us-cities-demographics  b99c18f8d5b344bd820a2606e57149a6ab91af26   \n",
       "\n",
       "   fields.total_population  fields.female_population  fields.count  \\\n",
       "0                   682545                  341408.0         72288   \n",
       "1                   115258                   59027.0          1916   \n",
       "2                   136454                   70240.0          7513   \n",
       "3                   214911                  112789.0          1500   \n",
       "4                   100883                   50091.0         92874   \n",
       "\n",
       "   fields.foreign_born fields.state_code  fields.average_household_size  \\\n",
       "0             113222.0                CO                           2.33   \n",
       "1              10925.0                UT                           3.28   \n",
       "2               6204.0                VA                           2.48   \n",
       "3               8258.0                AL                           2.21   \n",
       "4              11480.0                CO                           2.75   \n",
       "\n",
       "  fields.city                        fields.race  fields.male_population  \\\n",
       "0      Denver          Black or African-American                341137.0   \n",
       "1       Provo  American Indian and Alaska Native                 56231.0   \n",
       "2     Hampton                 Hispanic or Latino                 66214.0   \n",
       "3  Birmingham                              Asian                102122.0   \n",
       "4     Greeley                              White                 50792.0   \n",
       "\n",
       "   fields.median_age  fields.number_of_veterans fields.state  \n",
       "0               34.1                    29363.0     Colorado  \n",
       "1               23.6                     2177.0         Utah  \n",
       "2               35.5                    19638.0     Virginia  \n",
       "3               35.6                    13212.0      Alabama  \n",
       "4               31.0                     4294.0     Colorado  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We look a couple of records:\n",
    "data_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 14 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   datasetid                      2891 non-null   object \n",
      " 1   recordid                       2891 non-null   object \n",
      " 2   fields.total_population        2891 non-null   int64  \n",
      " 3   fields.female_population       2888 non-null   float64\n",
      " 4   fields.count                   2891 non-null   int64  \n",
      " 5   fields.foreign_born            2878 non-null   float64\n",
      " 6   fields.state_code              2891 non-null   object \n",
      " 7   fields.average_household_size  2875 non-null   float64\n",
      " 8   fields.city                    2891 non-null   object \n",
      " 9   fields.race                    2891 non-null   object \n",
      " 10  fields.male_population         2888 non-null   float64\n",
      " 11  fields.median_age              2891 non-null   float64\n",
      " 12  fields.number_of_veterans      2878 non-null   float64\n",
      " 13  fields.state                   2891 non-null   object \n",
      "dtypes: float64(6), int64(2), object(6)\n",
      "memory usage: 316.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Info about the columns data types:\n",
    "data_demographics.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll apply the next transformations to clean the dataset:\n",
    "\n",
    "* Drop the datasetid and recordid columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Airport Code Table: \n",
    "\n",
    "This data get the special code IATA and ICAO airtport code. To understand the structure and the definitions of the columns you can open the next link [Airport codes](https://datahub.io/core/airport-codes#data) contained in Datahub webpage. To this project, we take the info in json format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the current path to US demographic info: \n",
    "airport_path = os.path.join(os.getcwd() + \"/airport_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json dataset\n",
    "with open(airport_path + 'airport-codes_json.json', 'r') as f:\n",
    "    dataset_airport_codes = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, normalize the dataset:\n",
    "data_airport_code = pd.json_normalize(dataset_airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>continent</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>ident</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>local_code</th>\n",
       "      <th>municipality</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "      <td>11</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>00A</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>heliport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "      <td>3435</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>small_airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "      <td>450</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>00AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>small_airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NA</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "      <td>820</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>00AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>small_airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NA</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "      <td>237</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>00AR</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>None</td>\n",
       "      <td>Newport</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  continent                            coordinates elevation_ft gps_code  \\\n",
       "0        NA     -74.93360137939453, 40.07080078125           11      00A   \n",
       "1        NA                 -101.473911, 38.704022         3435     00AA   \n",
       "2        NA            -151.695999146, 59.94919968          450     00AK   \n",
       "3        NA  -86.77030181884766, 34.86479949951172          820     00AL   \n",
       "4        NA                    -91.254898, 35.6087          237     None   \n",
       "\n",
       "  iata_code ident iso_country iso_region local_code  municipality  \\\n",
       "0      None   00A          US      US-PA        00A      Bensalem   \n",
       "1      None  00AA          US      US-KS       00AA         Leoti   \n",
       "2      None  00AK          US      US-AK       00AK  Anchor Point   \n",
       "3      None  00AL          US      US-AL       00AL       Harvest   \n",
       "4      None  00AR          US      US-AR       None       Newport   \n",
       "\n",
       "                                 name           type  \n",
       "0                   Total Rf Heliport       heliport  \n",
       "1                Aero B Ranch Airport  small_airport  \n",
       "2                        Lowell Field  small_airport  \n",
       "3                        Epps Airpark  small_airport  \n",
       "4  Newport Hospital & Clinic Heliport         closed  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We look a couple of records:\n",
    "data_airport_code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57421 entries, 0 to 57420\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   continent     57421 non-null  object\n",
      " 1   coordinates   57421 non-null  object\n",
      " 2   elevation_ft  49608 non-null  object\n",
      " 3   gps_code      41561 non-null  object\n",
      " 4   iata_code     9225 non-null   object\n",
      " 5   ident         57421 non-null  object\n",
      " 6   iso_country   57421 non-null  object\n",
      " 7   iso_region    57421 non-null  object\n",
      " 8   local_code    30030 non-null  object\n",
      " 9   municipality  51527 non-null  object\n",
      " 10  name          57421 non-null  object\n",
      " 11  type          57421 non-null  object\n",
      "dtypes: object(12)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Info about the columns data types:\n",
    "data_airport_code.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NA    28443\n",
       "SA     8443\n",
       "EU     8404\n",
       "AS     5619\n",
       "AF     3361\n",
       "OC     3123\n",
       "AN       28\n",
       "Name: continent, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_airport_code['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NA    0.495341\n",
       "SA    0.147037\n",
       "EU    0.146358\n",
       "AS    0.097856\n",
       "AF    0.058533\n",
       "OC    0.054388\n",
       "AN    0.000488\n",
       "Name: continent, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_airport_code['continent'].value_counts('iata_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll apply the next transformations to clean the dataset:\n",
    "\n",
    "* Drop the datasetid and recordid columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
